{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yangyangkiki/LTU-CSE5CV-Term5-Labs/blob/main/Lab_05_Pedestrian_Tracking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5be53ebc-a4ba-4074-a978-130e90636931",
      "metadata": {
        "id": "5be53ebc-a4ba-4074-a978-130e90636931"
      },
      "source": [
        "# CSE5CV - Pedestrian Tracking\n",
        "\n",
        "In this lab we will create and use a pedestrian tracking algorithm.\n",
        "\n",
        "By the end of this lab, you should be able to:\n",
        "* Describe \"Association\" in the context of tracking\n",
        "* Understand the Linear Assignment Problem and it's relevance to tracking\n",
        "* Understand how you can use Mask RCNN with a simple tracking algorithm\n",
        "\n",
        "Evaluating tracking results is left to the reader."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab preparation\n",
        "\n",
        "Google Colab is a free online service for editing and running code in notebooks like this one. To get started, follow the steps below:\n",
        "\n",
        "1. Click the \"Copy to Drive\" button at the top of the page. This will open a new tab with the title \"Copy of...\". This is a copy of the lab notebook which is saved in your personal Google Drive. **Continue working in that copy, otherwise you will not be able to save your work**. You may close the original Colab page (the one which displays the \"Copy to Drive\" button).\n",
        "2. Run the code cell below to prepare the Colab coding environment by downloading sample files. Note that if you close this notebook and come back to work on it again later, you will need to run this cell again."
      ],
      "metadata": {
        "id": "qHrEAKS4h2Nm"
      },
      "id": "qHrEAKS4h2Nm"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ltu-cse5cv/cse5cv-labs.git\n",
        "%cd cse5cv-labs/Lab07"
      ],
      "metadata": {
        "id": "wM8UKEozh4MK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wM8UKEozh4MK"
    },
    {
      "cell_type": "markdown",
      "id": "5213d3ec-ef3c-4dc9-ba41-bce4a8834bbc",
      "metadata": {
        "id": "5213d3ec-ef3c-4dc9-ba41-bce4a8834bbc"
      },
      "source": [
        "## Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ae26e84-37e7-4ede-9f2c-db78536483a3",
      "metadata": {
        "id": "1ae26e84-37e7-4ede-9f2c-db78536483a3"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import scipy.optimize\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms.functional as tvtf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cce5cb59-d642-43b8-b585-f2ed879281d5",
      "metadata": {
        "id": "cce5cb59-d642-43b8-b585-f2ed879281d5"
      },
      "source": [
        "# 1. Brief disambiguation\n",
        "\n",
        "In this lab, we will use many terms, some with similar yet distinct meanings. Here we provide a disambiguation between these terms.\n",
        "\n",
        "**detection**: A detection is a single pair of coordinates or bounding box.\n",
        "\n",
        "**track**: A track is a construct in our code containing a list of detections. There can only ever be a single detection per frame of video per track. In this lab, it will take the form of a dictionary with two keys: `start_frame` and (`points` or `boxes`). Then `points[i]` is the `i`-th prediction of the track, and it was made on `start_frame+i`.\n",
        "\n",
        "**individual**: An individual is a physical object which is being imaged and tracked.\n",
        "\n",
        "**identity**: A track has a single identity. The identity is the predicted continuity of an individual through time created by collecting detections into a track. That is, if our tracking algorithm is not perfect, it may, in fact, describe the path of multiple individuals throughout the track. This type of error is called an \"identity swap\".\n",
        "\n",
        "**online algorithm**: An online algorithm is one which processes data linearly, in the order which it appears. Here, we mean that the algorithm will first consider all the detections on frame 1. Then it will consider all the detections on frame 2, and how they can link up. Then it will consider all the detections on frame 3, etc. This is in contrast to a \"global algorithm\" which would consider all the detections on all the frames all at the same time.\n",
        "\n",
        "**open**/**closed**: In an online tracking algorithm, when an individual is no longer being tracked, their track is marked as \"closed\" and won't be considered when choosing which tracks to add detections to. A track will be closed when we can no longer find detections to match it. This will happen if the individual goes out of frame or gets obscured by other things in the scene. The tracks which are actively being considered to add points to are called \"open\" tracks.\n",
        "\n",
        "**association**: In concept: We \"associate\" a detection with an identity. In code: On every frame of the video, we \"associate\" each detection with at most one of the open tracks. Some detections or tracks will not be associated if there is an unbalanced number of either."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e470f8c2-0420-45f5-87a0-57af610c3084",
      "metadata": {
        "id": "e470f8c2-0420-45f5-87a0-57af610c3084"
      },
      "outputs": [],
      "source": [
        "# Some examples in code:\n",
        "# Detections\n",
        "#  - Using points (x,y)\n",
        "point1 = [0.1, 0.2]\n",
        "point2 = [0.5, 0.3]\n",
        "point3 = [0.6, 0.8]\n",
        "#  - Using boxes (tlx, tly, brx, bry)\n",
        "box1 = [0.1, 0.1, 0.2, 0.2]\n",
        "box2 = [0.3, 0.1, 0.5, 0.15]\n",
        "\n",
        "# Track\n",
        "#  - Using Points\n",
        "points = [point1, point2, point3]\n",
        "point_track = {'start_frame': 0, 'points': points}\n",
        "#  - Using Boxes\n",
        "boxes = [box1, box2]\n",
        "box_track = {'start_frame': 0, 'boxes': boxes}\n",
        "\n",
        "# Plot the point_track\n",
        "def xy_separate(seq):\n",
        "    '''\n",
        "    Given a (seq)uence of [[x, y], [x, y], [x, y], ...] coordinates\n",
        "    separate them into two tuples (x, x, x, ...), (y, y, y, ...)\n",
        "    '''\n",
        "    return zip(*seq)\n",
        "\n",
        "fig, ax = plt.subplots(1, 1)\n",
        "x, y = xy_separate(point_track['points'])\n",
        "ax.plot(x, y)\n",
        "ax.set_xlim([0, 1])\n",
        "ax.set_ylim([0, 1])\n",
        "_ = ax.set_title(f'A track with {len(points)} points')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01f504cc-d708-43d1-9e29-d2ee52778681",
      "metadata": {
        "id": "01f504cc-d708-43d1-9e29-d2ee52778681"
      },
      "source": [
        "# 2. Tracking\n",
        "\n",
        "A tracking algorithm, in the context of video, is one which describes the path of an individual person/object within the video. There are two main approaches taken:\n",
        "1. **A \"tracker\"**: A \"tracker\" is an algorithm that uses the location of an object on frame `i`, and finds where that object is in frame `i+1`. Determining the initial position is usually done with an object detector on the first frame of the video, and then the \"tracker\" runs through the video, updating the location. This method does not extend well to multi-object tracking, where different objects come in and out of frame.\n",
        "2. **Tracking by detection**: There has been a lot of research into creating good object detectors. A video consists of many frames (images). So, if we run a very good object detector on each frame, then it is simply a matter of choosing which detections belong together. Tracking by detection typically runs much faster because it only looks at point data, rather than image data. This method's performance is limited by the performance of the object detector, but tends to be simpler to implement.\n",
        "\n",
        "We will use **Tracking by detection** in this lab as it is simpler for multi-object tracking and is more popular these days."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f2af33e-8b0d-4ad1-96c4-f290a18d2102",
      "metadata": {
        "id": "5f2af33e-8b0d-4ad1-96c4-f290a18d2102"
      },
      "source": [
        "# 3. Detections\n",
        "\n",
        "We will use MaskRCNN to produce the detections used for tracking. Since processing the whole video takes a long time, we have already written and run `get_detections` on the whole video and stored the result in `'./mot20-01-det.th'` for your convenience. Make sure you read and understand the code used:\n",
        "\n",
        "```python\n",
        "# From Lab 4\n",
        "def preprocess_image(image):\n",
        "    image = tvtf.to_tensor(image)\n",
        "    image = image.unsqueeze(dim=0)\n",
        "    return image\n",
        "\n",
        "# class_index==1 is person\n",
        "def get_detections(maskrcnn, vid, score_threshold=0.5, class_index=1):\n",
        "    ''' Runs maskrcnn over all frames in vid, storing the detections '''\n",
        "    # Record how long the video is (in frames)\n",
        "    vid_length = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "    \n",
        "    # Read the frame on which the video was previously\n",
        "    initial_frame = int(vid.get(cv2.CAP_PROP_POS_FRAMES))\n",
        "    \n",
        "    # Set the video the frame 0\n",
        "    vid.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
        "\n",
        "    # For each frame, read it, give it to maskrcnn and record the boxes\n",
        "    det = []\n",
        "    for i in range(vid_length):\n",
        "        _, img = vid.read()\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            result = maskrcnn(preprocess_image(img))[0]\n",
        "        # Only check for one class\n",
        "        mask1 = result[\"labels\"] == class_index\n",
        "        # And only if the model is confident enough about it\n",
        "        mask2 = result[\"scores\"] > score_threshold\n",
        "        mask = mask1 & mask2\n",
        "\n",
        "        boxes = result[\"boxes\"][mask].detach().cpu().numpy()\n",
        "        det.append(boxes)\n",
        "        print(f'{i+1:0d}/{vid_length}')\n",
        "\n",
        "    # Set the video back to the frame it was before we started\n",
        "    vid.set(cv2.CAP_PROP_POS_FRAMES, initial_frame)\n",
        "    \n",
        "    return det\n",
        "\n",
        "maskrcnn = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "maskrcnn.eval()\n",
        "vid = cv2.VideoCapture('./mot20-01.mp4')\n",
        "all_detections = get_detections(maskrcnn, vid, score_threshold=0.3)\n",
        "torch.save(all_detections, './mot20-01-det.th')\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31897964-9696-4cd7-b7c6-18255a83f351",
      "metadata": {
        "id": "31897964-9696-4cd7-b7c6-18255a83f351"
      },
      "outputs": [],
      "source": [
        "all_detections = torch.load('./mot20-01-det.th')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "976b69cc-612a-43fa-bdaf-1b2574c85535",
      "metadata": {
        "id": "976b69cc-612a-43fa-bdaf-1b2574c85535"
      },
      "source": [
        "## 3.1 Visualise\n",
        "\n",
        "We will do a sanity check to see how well MaskRCNN has detected the people.\n",
        "\n",
        "**Task**: Use the `cv2.VideoCapture` class (shown in the above code) to grab frames `[0, 10, 20, 30]` and the provided `draw_detections()` function to put boxes around all detected objects. Then draw each of these frames using `plt.imshow()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f23fb33-82e7-471e-abd7-2ca35ef88850",
      "metadata": {
        "id": "8f23fb33-82e7-471e-abd7-2ca35ef88850"
      },
      "outputs": [],
      "source": [
        "# From Lab 4 (with small modification; draw_detections no longer expects class predictions)\n",
        "COLOURS = [\n",
        "    tuple(int(colour_hex.strip('#')[i:i+2], 16) for i in (0, 2, 4))\n",
        "    for colour_hex in plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "]\n",
        "def draw_detections(img, det, colours=COLOURS):\n",
        "    for i, (tlx, tly, brx, bry) in enumerate(det):\n",
        "        i %= len(colours)\n",
        "        cv2.rectangle(img, (tlx, tly), (brx, bry), color=colours[i], thickness=2)\n",
        "\n",
        "# TODO: open video with cv2.VideoCapture\n",
        "# vid = ...\n",
        "\n",
        "frame_numbers = [0, 10, 20, 30]\n",
        "n_rows = len(frame_numbers)\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, 1, figsize=(22, 13*n_rows))\n",
        "for idx, fr_num in enumerate(frame_numbers):\n",
        "    # TODO: Set vid to frame i\n",
        "    # vid.set(...)\n",
        "\n",
        "    # TODO: Read a frame and convert the colours\n",
        "\n",
        "    # TODO: Get detections for frame i out of all_detections\n",
        "    # det = ...\n",
        "\n",
        "    # TODO: Change det to the np.int32 dtype\n",
        "    # det = ...\n",
        "\n",
        "    # TODO: Draw on img\n",
        "    # draw_detections(...)\n",
        "\n",
        "    # TODO: Uncomment\n",
        "    # axes[idx].imshow(img)\n",
        "    axes[idx].axis('off')\n",
        "    axes[idx].set_title(f'Frame #{i}')\n",
        "plt.savefig('./detections.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbb494a6-44d7-4bac-97c7-0a7a14901ae9",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "cbb494a6-44d7-4bac-97c7-0a7a14901ae9"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "# TODO: open video with cv2.VideoCapture\n",
        "vid = cv2.VideoCapture('./mot20-01.mp4')\n",
        "\n",
        "fig, axes = plt.subplots(n_rows, 1, figsize=(22, 13*n_rows))\n",
        "for idx, fr_num in enumerate(frame_numbers):\n",
        "    # TODO: Set vid to frame i\n",
        "    vid.set(cv2.CAP_PROP_POS_FRAMES, fr_num)\n",
        "\n",
        "    # TODO: Read a frame\n",
        "    _, img = vid.read()\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # TODO: Get detections for frame i out of all_detections\n",
        "    det = all_detections[fr_num]\n",
        "\n",
        "    # TODO: Change det to the np.int32 dtype\n",
        "    det = det.astype(np.int32)\n",
        "\n",
        "    # TODO: Draw on img\n",
        "    draw_detections(img, det)\n",
        "\n",
        "    axes[idx].imshow(img)\n",
        "    axes[idx].axis('off')\n",
        "    axes[idx].set_title(f'Frame #{fr_num}')\n",
        "plt.savefig('./detections.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e2a3eb0-3daa-46f6-b940-33d6266dd66a",
      "metadata": {
        "id": "6e2a3eb0-3daa-46f6-b940-33d6266dd66a"
      },
      "source": [
        "**Question**: Looking at the three people in the bottom left of the frames. They are all consistently detected across frames. However, their boxes are drawn in different colours on each frame. Why is this?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "We don't actually know that these detections belong to the same people, yet. All we know is that MaskRCNN told us where a bunch of people were on each frame. More specifically, remember that MaskRCNN orders it's predictions by confidence (score), and <code>draw_detections</code> just assigns the colour by the order.\n",
        "</details>\n",
        "<br/>\n",
        "\n",
        "**Question**: Can you see any false negatives in the above image? What about false positives?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "A false negative is where there is actually a person, but MaskRCNN did not detect them. There are several examples of this on each frame at the very back, where it seems the people are too small to be detected reliably.\n",
        "\n",
        "A false positive is where MaskRCNN predicted a person, but there wasn't one. It's hard to spot, but there is at least one false positive on the middle of the right-hand-side of the image for all of the frames.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5757d23-7d94-4bce-9bd3-d3c0fff659f4",
      "metadata": {
        "id": "a5757d23-7d94-4bce-9bd3-d3c0fff659f4"
      },
      "source": [
        "# 4. Association\n",
        "\n",
        "The process by which we determine that a point/bounding box belongs to a track is called \"association\". There are two steps in associating tracks with detections:\n",
        "\n",
        "1. Measure cost.\n",
        "2. Choose pairings between detection and track with minimal cost."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "253d1393-ed5c-417c-9962-48a30d634d1a",
      "metadata": {
        "id": "253d1393-ed5c-417c-9962-48a30d634d1a"
      },
      "source": [
        "## 4.1 Measure Cost\n",
        "\n",
        "The first step of association is measuring the cost of adding each detection to each track. Depending on your data, there may be different ways to measure how well a detection fits with an existing track. In the below code cell, we define two tracks and two detections. We're going to use a bit of intuition here, then go through exact methods after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a4df8de-4041-499d-9319-98d89191f401",
      "metadata": {
        "id": "5a4df8de-4041-499d-9319-98d89191f401"
      },
      "outputs": [],
      "source": [
        "detections = [\n",
        "    [0.7, 0.7],\n",
        "    [0.1, 0.1],\n",
        "    [2.5, 2.1],\n",
        "    [5.2, 7.7]\n",
        "]\n",
        "\n",
        "tracks = [\n",
        "    {'start_frame': 0, 'points': [\n",
        "        [0.25, 0.25],\n",
        "        [0.20, 0.20],\n",
        "        [0.15, 0.15]\n",
        "    ]},\n",
        "    {'start_frame': 0, 'points': [\n",
        "        [0.55, 0.55],\n",
        "        [0.60, 0.60],\n",
        "        [0.65, 0.65],\n",
        "    ]},\n",
        "    {'start_frame': 0, 'points': [\n",
        "        [2.8, 2.4],\n",
        "        [2.7, 2.3],\n",
        "        [2.6, 2.2],\n",
        "    ]}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85ddaf29-5fa4-48ab-8701-4dc4a779101e",
      "metadata": {
        "id": "85ddaf29-5fa4-48ab-8701-4dc4a779101e"
      },
      "source": [
        "**Question**: Are the `detections` above points or boxes?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "A single video frame has a width and a height. A pair of coordinates has an x and a y. An axis-aligned box is represented with 4 numbers. Thus, the <code>detections</code> are points, as there is only one pair of coordinates per detection.\n",
        "</details>\n",
        "<br />\n",
        "\n",
        "**Question**: What is the frame number of the hypothetical video frame that these `detections` are for?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "All tracks start at frame 0, and have 3 points (for frames [0,1,2]), thus the current <code>detections</code> are for frame 3.\n",
        "</details>\n",
        "<br />\n",
        "\n",
        "**Question**: Which detection belongs with which track? i.e. Should `detections[0]` be associated with `tracks[0]`, `tracks[1]` or `tracks[2]`?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "We are looking for a next point in each track which continues the existing pattern. We can see that `tracks[1]` is increasing linearly towards `detections[0]`. Thus we would say that we should associate `detections[0]` with `tracks[1]`. And similarly from `tracks[0]` to `detections[1]`, etc.\n",
        "</details>\n",
        "<br />\n",
        "\n",
        "**Task**: Add the detections to the tracks as per the answer to the above question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a120420d-6820-40f3-bffc-026307378f13",
      "metadata": {
        "id": "a120420d-6820-40f3-bffc-026307378f13"
      },
      "outputs": [],
      "source": [
        "# Make a copy so we don't mess with the original\n",
        "tracks_copy = copy.deepcopy(tracks)\n",
        "\n",
        "# TODO: add detections to tracks_copy\n",
        "\n",
        "print(tracks_copy[0]['points'])\n",
        "print(tracks_copy[1]['points'])\n",
        "assert len(tracks_copy[0]['points']) == 4\n",
        "assert len(tracks_copy[1]['points']) == 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5ecc2518-4a30-4b91-9099-06b2f36f64dc",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "5ecc2518-4a30-4b91-9099-06b2f36f64dc"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "# Make a copy so we don't mess with the original\n",
        "tracks_copy = copy.deepcopy(tracks)\n",
        "\n",
        "tracks_copy[1]['points'].append(detections[0])\n",
        "tracks_copy[0]['points'].append(detections[1])\n",
        "\n",
        "print(tracks_copy[0]['points'])\n",
        "print(tracks_copy[1]['points'])\n",
        "assert len(tracks_copy[0]['points']) == 4\n",
        "assert len(tracks_copy[1]['points']) == 4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e23f017-f98b-4107-9c97-e5058170ba64",
      "metadata": {
        "id": "6e23f017-f98b-4107-9c97-e5058170ba64"
      },
      "source": [
        "### 4.1.1 Point-point distance\n",
        "\n",
        "The simplest cost is the Euclidean distance between the last point of the track and the detected point. We can easily do this with the [`np.linalg.norm()` function](https://numpy.org/doc/1.20/reference/generated/numpy.linalg.norm.html).\n",
        "\n",
        "*Note*: Euclidean distance and L2 norm are the same thing.\n",
        "\n",
        "**Task**: Calculate the distance between `p1` and `p2` with basic arithmetic operators (e.g. `+`, `-`, `*`, `/`, `**`). Separately, use the [`np.linalg.norm()` function](https://numpy.org/doc/1.20/reference/generated/numpy.linalg.norm.html) to calculate the distance, and compare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a453d33c-be99-4a4a-80d9-80068bf9269d",
      "metadata": {
        "id": "a453d33c-be99-4a4a-80d9-80068bf9269d"
      },
      "outputs": [],
      "source": [
        "p1 = [0.10, 0.10]\n",
        "p2 = [0.15, 0.15]\n",
        "diff = np.array(p2)-np.array(p1)\n",
        "\n",
        "# TODO: Calculate manually\n",
        "# dist_manual = ...\n",
        "# print(dist_manual)\n",
        "\n",
        "# TODO: Calculate using np.linalg.norm\n",
        "# dist_np = ...\n",
        "# print(dist_np)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfb5287e-fc9c-4019-b478-453d7d564fe8",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "dfb5287e-fc9c-4019-b478-453d7d564fe8"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "# TODO: Calculate manually\n",
        "dist_manual = (diff[0]**2 + diff[1]**2)**0.5\n",
        "print(dist_manual)\n",
        "\n",
        "# TODO: Calculate using np.linalg.norm\n",
        "dist_np = np.linalg.norm(diff)\n",
        "print(dist_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f3f4859-36f7-4c8a-89bc-708f5a1757bc",
      "metadata": {
        "id": "3f3f4859-36f7-4c8a-89bc-708f5a1757bc"
      },
      "source": [
        "Now that we know a concrete method for determining a cost of joining tracks and points...\n",
        "\n",
        "**Task**: Calculate the euclidean distance from `detections[0]` to the ends of both `tracks[0]` and `tracks[1]` using `np.linalg.norm()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a84d1606-dba5-4bb6-8e04-a3c59ce14767",
      "metadata": {
        "id": "a84d1606-dba5-4bb6-8e04-a3c59ce14767"
      },
      "outputs": [],
      "source": [
        "diff00 = np.array(detections[0])-np.array(tracks[0]['points'][-1])\n",
        "diff01 = np.array(detections[0])-np.array(tracks[1]['points'][-1])\n",
        "\n",
        "# TODO\n",
        "# dist00 = ...\n",
        "# dist01 = ...\n",
        "\n",
        "print(dist00)\n",
        "print(dist01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cd9e99c-acce-43b1-bb74-0b18bbb9ea3a",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "1cd9e99c-acce-43b1-bb74-0b18bbb9ea3a"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "dist00 = np.linalg.norm(diff00)\n",
        "dist01 = np.linalg.norm(diff01)\n",
        "\n",
        "print(dist00)\n",
        "print(dist01)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "238475c0-b291-40b9-99f6-007ddf444a05",
      "metadata": {
        "id": "238475c0-b291-40b9-99f6-007ddf444a05"
      },
      "source": [
        "We can see that the euclidean distance between `detections[0]` and `tracks[1]` is the smaller value. This tells us that `detections[0]` should be added to `tracks[1]`, just like the intuition."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "833e6a5f-bc3c-49fe-99c3-7be12e7235cb",
      "metadata": {
        "id": "833e6a5f-bc3c-49fe-99c3-7be12e7235cb"
      },
      "source": [
        "#### Vectorisation\n",
        "\n",
        "In this case we have four detections and three tracks, and we need to compare each detection with each track.\n",
        "\n",
        "**Question**: How many distances will we need to calculate to compare all detections with all tracks?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "We will need to calculate 12 different distances. For every detection (4), we must measure the distance with every track (3), so there are (4*3=12) different distances to compare. We will typically store these as a <code>np.array</code> shaped <code>[4, 3]</code>\n",
        "</details>\n",
        "<br/>\n",
        "\n",
        "We would like to write code that can compare an arbitrary number of detections and tracks with minimal code. We could write a for loop, but we can avoid that by using [broadcasting](https://numpy.org/doc/1.20/user/basics.broadcasting.html) in numpy. This is called \"vectorisation\" and runs significantly faster than using a for-loop. In the code cell below, we show finding a difference between all detections and all tracks using broadcasting.\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Extra details: Using <code>arr[None]</code></u></summary>\n",
        "\n",
        "In numpy, using a `None` in the index inserts a singleton dimension at that position. e.g. if `arr` is shaped `[3,4,5]` then `arr[:, None]` is shaped `[3,1,4,5]`. It is a shorthand for using something like [np.reshape()](https://numpy.org/doc/1.21/reference/generated/numpy.reshape.html) or [np.expand_dims()](https://numpy.org/doc/1.21/reference/generated/numpy.expand_dims.html) to insert that singleton dimension.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f2bbdc-7c7e-493d-95ca-dc0c01140197",
      "metadata": {
        "id": "32f2bbdc-7c7e-493d-95ca-dc0c01140197"
      },
      "outputs": [],
      "source": [
        "detections_np = np.array(detections)\n",
        "# A numpy array of the last point in each track\n",
        "previous_np = np.array([t['points'][-1] for t in tracks])\n",
        "\n",
        "print('Shapes before')\n",
        "print(detections_np.shape)\n",
        "print(previous_np.shape)\n",
        "\n",
        "detections_np_expanded = detections_np[:, None]\n",
        "previous_np_expanded = previous_np[None]\n",
        "\n",
        "print('Shapes after expanding')\n",
        "print(detections_np_expanded.shape)\n",
        "print(previous_np_expanded.shape)\n",
        "\n",
        "diff = detections_np_expanded - previous_np_expanded\n",
        "\n",
        "print('Shape of diff')\n",
        "print(diff.shape)\n",
        "\n",
        "# This is a common numpy pattern. The above code is typically done in one line:\n",
        "diff = detections_np[:, None] - previous_np[None]\n",
        "print(diff.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e61e5445-0fe5-4aa6-ac91-90e72fe3c299",
      "metadata": {
        "id": "e61e5445-0fe5-4aa6-ac91-90e72fe3c299"
      },
      "source": [
        "**Question**: Notice that the shape of `diff` is `(<n_detections>, <n_tracks>, 2)`. Why is the last dimension 2?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "<code>diff</code> Is a 1D difference in both the <code>x</code> and the <code>y</code>. The last dimension is just the <code>x</code> and the <code>y</code>.\n",
        "</details>\n",
        "<br/>\n",
        "\n",
        "**Question**: By indexing into `diff`. How do we find the difference in the `x` dimension between the `detections[2]` and the `tracks[0]`.\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "<code>diff[2, 0, 0]</code>\n",
        "</details>\n",
        "<br/>\n",
        "\n",
        "**Question**: By indexing into `diff`. How do we find the distance between the `detections[2]` and the `tracks[0]`.\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "<code>np.linalg.norm(diff[2, 0])</code>\n",
        "</details>\n",
        "<br/>\n",
        "\n",
        "**Task**: And the last step: calculate the distance between all detections and all tracks using `np.linalg.norm()` in a single line of code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d15e290-f21e-4d63-b00a-6ac1fe748cef",
      "metadata": {
        "id": "4d15e290-f21e-4d63-b00a-6ac1fe748cef"
      },
      "outputs": [],
      "source": [
        "# TODO\n",
        "# dist = ...\n",
        "print(dist.shape)\n",
        "print(dist[2, 0])\n",
        "print(np.linalg.norm(diff[2, 0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "006deeb5-1cbb-47ba-8ed4-89538e9d1b97",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "006deeb5-1cbb-47ba-8ed4-89538e9d1b97"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "dist = np.linalg.norm(diff, axis=-1)\n",
        "print(dist.shape)\n",
        "print(dist[2, 0])\n",
        "print(np.linalg.norm(diff[2, 0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f69aac96-7415-4005-a17b-339227ff5715",
      "metadata": {
        "id": "f69aac96-7415-4005-a17b-339227ff5715"
      },
      "source": [
        "### 4.1.2 Bounding box center distance\n",
        "\n",
        "Point-point distance measurement is the most common cost metric. It is sometimes preferred to convert bounding boxes into a single point so that we can use Euclidean point-point distance.\n",
        "\n",
        "**Task**: Convert `boxes` in the below code into a list of points that are the center coordinates of the bounding boxes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6a2fcee-7d21-4d07-a59e-ad3f7c3dd19f",
      "metadata": {
        "id": "e6a2fcee-7d21-4d07-a59e-ad3f7c3dd19f"
      },
      "outputs": [],
      "source": [
        "boxes = [\n",
        "    [0.1, 0.1, 0.2, 0.2],\n",
        "    [0.3, 0.1, 0.5, 0.15],\n",
        "    [0.2, 0.6, 0.4, 0.8],\n",
        "]\n",
        "\n",
        "# TODO: complete function\n",
        "def tlbr_to_center(boxes):\n",
        "    points = []\n",
        "    for tlx, tly, brx, bry in boxes:\n",
        "        pass\n",
        "        # TODO: convert to cx, cy\n",
        "    return points\n",
        "\n",
        "print(tlbr_to_center(boxes))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0534428b-e19f-4cb2-bac1-ebfd873ce199",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "0534428b-e19f-4cb2-bac1-ebfd873ce199"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "def tlbr_to_center(boxes):\n",
        "    points = []\n",
        "    for tlx, tly, brx, bry in boxes:\n",
        "        cx = (tlx+brx)/2\n",
        "        cy = (tly+bry)/2\n",
        "        points.append((cx, cy))\n",
        "    return points\n",
        "\n",
        "print(tlbr_to_center(boxes))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39b3567a-2add-406a-adc0-e67b1b39c0f8",
      "metadata": {
        "id": "39b3567a-2add-406a-adc0-e67b1b39c0f8"
      },
      "source": [
        "### 4.1.3 Bounding box overlap\n",
        "\n",
        "If we are using bounding boxes, then we can of course simply measure similarity based on the overlap between the boxes. We've done this before.\n",
        "\n",
        "**Question**: How do we typically measure similarity between bounding boxes?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer and next task</u></summary>\n",
        "We usually use Intersection over Union (IoU) or Dice Coefficient (DSC) to measure similarity between bounding boxes.\n",
        "\n",
        "**Task**: Use `bbox_iou_matrix()` to measure the similarity between `detections` and `tracks` in the below code.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8bc87631-8a6a-43dd-8d7f-ab8e6e99d4cd",
      "metadata": {
        "id": "8bc87631-8a6a-43dd-8d7f-ab8e6e99d4cd"
      },
      "outputs": [],
      "source": [
        "# Used in Lab 04\n",
        "def bbox_iou_matrix(a, b):\n",
        "    '''\n",
        "    Vectorised iou calculation for bbox predictions.\n",
        "\n",
        "    Parameters:\n",
        "        a (np.array [N, 4]): array of N boxes, each as [tlx, tly, brx, bry]\n",
        "        b (np.array [M, 4]): as `a`, but with M boxes\n",
        "\n",
        "    Returns:\n",
        "        np.array [N, M]: the iou for every pair of boxes.\n",
        "    '''\n",
        "    a = a[:, None] # [N, 1, 4]\n",
        "    b = b[None, :] # [1, M, 4]\n",
        "\n",
        "    tlx_a, tly_a, brx_a, bry_a = [a[..., i] for i in range(4)]\n",
        "    tlx_b, tly_b, brx_b, bry_b = [b[..., i] for i in range(4)]\n",
        "\n",
        "    # Find the overlap/intersection\n",
        "    # note: np.maximum() != np.max()\n",
        "    tlx_overlap = np.maximum(tlx_a, tlx_b)\n",
        "    tly_overlap = np.maximum(tly_a, tly_b)\n",
        "    brx_overlap = np.minimum(brx_a, brx_b)\n",
        "    bry_overlap = np.minimum(bry_a, bry_b)\n",
        "    # Clipping to account for non-overlapping\n",
        "    intersection = (brx_overlap - tlx_overlap).clip(0) * (bry_overlap - tly_overlap).clip(0)\n",
        "\n",
        "    area_a = abs((brx_a - tlx_a) * (bry_a - tly_a))\n",
        "    area_b = abs((brx_a - tlx_a) * (bry_a - tly_a))\n",
        "    union = area_a + area_b - intersection\n",
        "\n",
        "    return intersection / union\n",
        "\n",
        "\n",
        "\n",
        "detections = [\n",
        "    [0.1, 0.1, 0.2, 0.2],\n",
        "    [0.3, 0.1, 0.5, 0.2],\n",
        "    [0.2, 0.5, 0.3, 0.7],\n",
        "    [0.6, 0.2, 0.9, 0.3],\n",
        "]\n",
        "\n",
        "tracks = [\n",
        "    {'start_frame': 0, 'boxes': [\n",
        "        [0.08, 0.08, 0.18, 0.18],\n",
        "        [0.09, 0.09, 0.19, 0.19],\n",
        "    ]},\n",
        "    {'start_frame': 0, 'boxes': [\n",
        "        [0.22, 0.52, 0.32, 0.72],\n",
        "        [0.21, 0.51, 0.31, 0.71],\n",
        "    ]},\n",
        "]\n",
        "\n",
        "# TODO\n",
        "# detections_np = ...\n",
        "# previous_np = ...\n",
        "# iou = bbox_iou_matrix(...)\n",
        "# assert iou.shape == (4,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee3f262d-8869-4ad6-84d9-c332943b5d81",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "ee3f262d-8869-4ad6-84d9-c332943b5d81"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "detections_np = np.array(detections)\n",
        "previous_np = np.array([t['boxes'][-1] for t in tracks])\n",
        "\n",
        "iou = bbox_iou_matrix(detections_np, previous_np)\n",
        "assert iou.shape == (4,2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33823257-4c65-4613-98df-4b3d4163eeb8",
      "metadata": {
        "id": "33823257-4c65-4613-98df-4b3d4163eeb8"
      },
      "source": [
        "**Question**: Which `detections` should be associated with which `tracks`? Make your estimation, print out `iou` and check your answer.\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "Printing out `iou` gives:\n",
        "\n",
        "```\n",
        "[[0.68067227 0.        ]\n",
        " [0.         0.        ]\n",
        " [0.         0.74672489]\n",
        " [0.         0.        ]]\n",
        "```\n",
        "\n",
        "This tells us that `detections[0]` should be associated with `tracks[0]`, `detections[2]` should be associated with `tracks[1]`, and the remaining detections are not associated with any track. *Note*: A \"cost\" should be low to indicate that a detection should be matched with a track. So when using IoU as a cost metric, make sure you use `(1-iou)`.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10a2f99c-fa92-43bf-b6f9-f3ae8242658f",
      "metadata": {
        "id": "10a2f99c-fa92-43bf-b6f9-f3ae8242658f"
      },
      "source": [
        "### 4.1.4 Appearance features\n",
        "\n",
        "Technically speaking, tracking by detection is under-defined. In many circumstances there are multiple plausible associations. Sometimes we need to use the information in the actual image to decide how to join. For example, if we see one track is following someone wearing a red shirt and we have two plausible detections, one which covers someone with a green shirt and the other a red shirt, then we can conclude we should join the detection for someone wearing a red shirt to our existing track, even if it's further away.\n",
        "\n",
        "Following this intuition, we want a systematic way to measure similarity between image patches. We already know that comparing images pixel-by-pixel isn't a good idea.\n",
        "\n",
        "**Question**: How do we reduce the dimensionality of an image?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "We usually use image features to reduce the dimensionality of images for further processing. This is true here, too.\n",
        "</details>\n",
        "<br/>\n",
        "\n",
        "**Task**: Extract the first two colour moments from the provided crops.\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Extra details: This is a bit weird</u></summary>\n",
        "At the beginning we said that we preferred tracking by detection because it was simpler and didn't need to look at the images. But now we are saying that we can look at the images to get information we couldn't get otherwise. And, most \"trackers\" approaches use primarily appearance features to match between frames. The distinction between a \"tracker\" approach and a \"tracking by detection\" approach isn't very clear-cut once we start using appearance features.\n",
        "\n",
        "In practice it's up to you to figure out which is more appropriate for your situation, and which describes your approach better.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fbef712-9c39-407f-98ec-da8f3784310d",
      "metadata": {
        "id": "7fbef712-9c39-407f-98ec-da8f3784310d"
      },
      "outputs": [],
      "source": [
        "# Get two early frames of video\n",
        "vid = cv2.VideoCapture('./mot20-01.mp4')\n",
        "_, first_frame = vid.read()\n",
        "first_frame = cv2.cvtColor(first_frame, cv2.COLOR_BGR2RGB)\n",
        "for x in range(3):\n",
        "    vid.read()\n",
        "_, second_frame = vid.read()\n",
        "second_frame = cv2.cvtColor(second_frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "# Get first maskrcnn detection of first two frames\n",
        "first_frame_det = all_detections[0].astype(np.int32)\n",
        "second_frame_det = all_detections[1].astype(np.int32)\n",
        "\n",
        "# Two crops of two frames\n",
        "tlx, tly, brx, bry = first_frame_det[0]\n",
        "cropped00 = first_frame[tly:bry, tlx:brx]\n",
        "tlx, tly, brx, bry = first_frame_det[1]\n",
        "cropped01 = first_frame[tly:bry, tlx:brx]\n",
        "tlx, tly, brx, bry = second_frame_det[0]\n",
        "cropped40 = second_frame[tly:bry, tlx:brx]\n",
        "tlx, tly, brx, bry = second_frame_det[1]\n",
        "cropped41 = second_frame[tly:bry, tlx:brx]\n",
        "\n",
        "def do_show(ax, img, title):\n",
        "    ax.imshow(img)\n",
        "    ax.axis('off')\n",
        "    ax.set_title(title)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(8, 10))\n",
        "do_show(axes[0, 0], cropped00, 'Frame #0, Crop #0')\n",
        "do_show(axes[0, 1], cropped01, 'Frame #0, Crop #1')\n",
        "do_show(axes[1, 0], cropped40, 'Frame #4, Crop #0')\n",
        "do_show(axes[1, 1], cropped41, 'Frame #4, Crop #1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b2647d0-3a2f-4749-8aa3-f7755f3bcc81",
      "metadata": {
        "id": "0b2647d0-3a2f-4749-8aa3-f7755f3bcc81"
      },
      "outputs": [],
      "source": [
        "# TODO: Extract first two colour moments from all crops\n",
        "# Hint: Concatentate each into a single 6D vector to represent the crop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e99a66b6-4b76-4f86-a769-8a47d0c5cc06",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "e99a66b6-4b76-4f86-a769-8a47d0c5cc06"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "def get_features(cropped):\n",
        "    return np.concatenate([\n",
        "        cropped.mean(axis=(0, 1)),\n",
        "        cropped.std(axis=(0, 1))\n",
        "    ])\n",
        "\n",
        "crop00_features = get_features(cropped00)\n",
        "crop01_features = get_features(cropped01)\n",
        "crop40_features = get_features(cropped40)\n",
        "crop41_features = get_features(cropped41)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f699413c-aa90-4e7e-82ee-c9a4ce727f6e",
      "metadata": {
        "id": "f699413c-aa90-4e7e-82ee-c9a4ce727f6e"
      },
      "source": [
        "We now have a feature vector to represent all four crops. That is, we have mapped the crops to a 6D point in \"feature space\". We have already shown you how to measure the distance between points.\n",
        "\n",
        "**Task**: Use `np.linalg.norm()` to measure the distance between each crop on frame 0 and each crop on frame 4 in feature space using vectorisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40ad5122-e0f7-41ce-9469-d804fdb37956",
      "metadata": {
        "id": "40ad5122-e0f7-41ce-9469-d804fdb37956"
      },
      "outputs": [],
      "source": [
        "# TODO:\n",
        "# frame_0_crops = ...\n",
        "# frame_4_crops = ...\n",
        "# dist = ...\n",
        "print(dist)\n",
        "print(dist[0, 1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2bd930a5-6bc4-443c-90b8-b8e23f76785e",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "2bd930a5-6bc4-443c-90b8-b8e23f76785e"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "frame_0_crop_features = np.stack([crop00_features, crop01_features])\n",
        "frame_4_crop_features = np.stack([crop40_features, crop41_features])\n",
        "dist = np.linalg.norm(frame_0_crop_features[:, None] - frame_4_crop_features[None], axis=-1)\n",
        "print(dist)\n",
        "print(dist[0, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ec399c1-622a-43b7-810f-4b9205dce869",
      "metadata": {
        "id": "4ec399c1-622a-43b7-810f-4b9205dce869"
      },
      "source": [
        "**Question**: Which pair of crops have the most similar image features? What does this tell us?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "We can see that dist[0, 1] is the smallest diff. This tells us that crop \\#0 from frame \\#0 is most similar to crop \\#1 from frame \\#4. We can visually check that this answer makes sense. It is clear that these crops show the same woman with a green shirt and white handbag. This tells us that these detections should belong to the same track.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Extra details: Other image features</u></summary>\n",
        "\n",
        "We've only used very simple image features for this. In theory, you can use any image features to describe your crops, but so long as you map all of your crops into the same feature space, you can always calculate the distance between them the same way.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Extra details: Euclidean distance in image</u></summary>\n",
        "\n",
        "We generally don't want to completely ignore where the crops are in an image, else we may join bounding boxes on the other side of the image. So, we could typically use a linear combination of the Euclidean distance and appearance feature distance. e.g. `cost = 0.5*euclidean_distance + 0.5*feature_distance`. Intuitively we are saying \"crops that are close and look similar should belong together\". The exact proportion of each is not standardised.\n",
        "</details>\n",
        "\n",
        "<br />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a360a369-c2f5-4401-87de-f293efd3ec9e",
      "metadata": {
        "id": "a360a369-c2f5-4401-87de-f293efd3ec9e"
      },
      "source": [
        "## 4.2 Minimal cost\n",
        "\n",
        "Now that we have described methods for measuring the cost for associating a detection to a track, we need to describe a method for selecting which detection to attach to each track. But, hold on, that's easy, isn't it? Just choose the smallest cost, right? It turns out to not be quite that simple."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "005ceb6d-98cd-4883-a243-03831a0efbfd",
      "metadata": {
        "id": "005ceb6d-98cd-4883-a243-03831a0efbfd"
      },
      "source": [
        "### 4.2.1 Greedy\n",
        "\n",
        "A greedy algorithm is one which looks at the available options and iteratively picks the smallest cost. This does not work well for association in general. Let's explore an example to see why. Suppose we have these detections:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21d09fd0-c9d1-4d43-9428-f8a690039730",
      "metadata": {
        "id": "21d09fd0-c9d1-4d43-9428-f8a690039730"
      },
      "outputs": [],
      "source": [
        "detections = np.array([\n",
        "    [[0, 1], [4, 1]],\n",
        "    [[4, 4], [8, 4]],\n",
        "])\n",
        "\n",
        "cost = np.linalg.norm(detections[0][:, None] - detections[1][None], axis=-1)\n",
        "print(cost)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87b3f48a-844e-44eb-8433-63dba52cac8b",
      "metadata": {
        "id": "87b3f48a-844e-44eb-8433-63dba52cac8b"
      },
      "source": [
        "The smallest distance is `cost[1, 0]`, which is saying the \"best\" option is to join `detections[0][1]` (i.e. `[4, 1]`) with `detections[1][0]` (`[4, 4]`). So, using a greedy approach we would create a track: `[[4,1], [4,4]]`. Then, we would be left joining the remaining two points into a second track `[[0, 1], [8, 4]]`. But those are really far away! Here's a visualisation of what we just did."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd2092b-9399-40c7-a30d-05401413d5bc",
      "metadata": {
        "id": "0bd2092b-9399-40c7-a30d-05401413d5bc"
      },
      "outputs": [],
      "source": [
        "# Draw detections as dots\n",
        "N = len(detections)\n",
        "for i, coords in enumerate(detections):\n",
        "    x, y = xy_separate(coords)\n",
        "    g = i % 2\n",
        "    plt.scatter(x, y, color=(0, (1-g), g), s=300)\n",
        "\n",
        "track1 = {'first_frame': 0, 'points': [[4, 1], [4, 4]]}\n",
        "track2 = {'first_frame': 0, 'points': [[0, 1], [8, 4]]}\n",
        "\n",
        "# Draw line\n",
        "x, y = xy_separate(track1['points'])\n",
        "plt.plot(x, y)\n",
        "x, y = xy_separate(track2['points'])\n",
        "_ = plt.plot(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "109d0e9b-a555-4c82-8b71-f3ce04388a44",
      "metadata": {
        "id": "109d0e9b-a555-4c82-8b71-f3ce04388a44"
      },
      "source": [
        "Oh dear! That doesn't look right. The problem is that when you greedily assign detections to tracks, you can be left only with very bad options after you've used the best ones. Let's look again at the cost matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c846321f-49a5-47bb-a776-55a1b100ddd1",
      "metadata": {
        "id": "c846321f-49a5-47bb-a776-55a1b100ddd1"
      },
      "outputs": [],
      "source": [
        "print(cost)\n",
        "print('The total cost with a greedy approach: ', cost[1, 0] + cost[0, 1])\n",
        "print('The optimum total cost:                ', cost[0, 0] + cost[1, 1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "116d6556-29ec-40b7-b42f-5fdcd6b91d17",
      "metadata": {
        "id": "116d6556-29ec-40b7-b42f-5fdcd6b91d17"
      },
      "source": [
        "We can see that the cost of putting `[0, 1]` in the same track as `[5, 2]` is so large that it outweighs the benefit of choosing to put `[3, 1]` with `[2, 2]`. To resolve this we will need to find a globally optimum assignment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd6c3776-137b-4143-ade2-ba9b48690bd8",
      "metadata": {
        "id": "cd6c3776-137b-4143-ade2-ba9b48690bd8"
      },
      "source": [
        "### 4.2.2 Linear Assignment Problem\n",
        "\n",
        "Outside of tracking, this problem has already been studied, and good algorithms proposed to solve this. It is known as a [Linear Assignment Problem](https://en.wikipedia.org/wiki/Assignment_problem).\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Extra details: Other names</u></summary>\n",
        "\n",
        "This problem is very generic, finding uses well outside the realm of computer vision. The scipy function is called `linear_sum_assignment`, and it says that it implements a solution to \"minimum weight matching in bipartite graphs\". The problem can also be formulated as something called a minimum cost flow network, but the solution is the same for this formulation.\n",
        "</details>\n",
        "<br />\n",
        "\n",
        "**Task**: Use the [`scipy.optimize.linear_sum_assignment()` function](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linear_sum_assignment.html) to find the globally optimal assignments. Use the result to assign the `detections[1]` to the correct `tracks`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33f918a7-7147-4a8c-9f61-0a771c0fd902",
      "metadata": {
        "id": "33f918a7-7147-4a8c-9f61-0a771c0fd902"
      },
      "outputs": [],
      "source": [
        "tracks = [\n",
        "    {'start_frame': 0, 'points':[detections[0][0]]},\n",
        "    {'start_frame': 0, 'points':[detections[0][1]]},\n",
        "]\n",
        "# TODO: use scipy.optimize.linear_sum_assignment to add points from detections[1]\n",
        "\n",
        "\n",
        "# Draw detections as dots\n",
        "N = len(detections)\n",
        "for i, coords in enumerate(detections):\n",
        "    x, y = xy_separate(coords)\n",
        "    g = i % 2\n",
        "    plt.scatter(x, y, color=(0, (1-g), g), s=300)\n",
        "\n",
        "# Draw line\n",
        "x, y = xy_separate(tracks[0]['points'])\n",
        "plt.plot(x, y)\n",
        "x, y = xy_separate(tracks[1]['points'])\n",
        "_ = plt.plot(x, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "491a48c1-59aa-4152-8661-ae0c38745748",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "491a48c1-59aa-4152-8661-ae0c38745748"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "tracks = [\n",
        "    {'start_frame': 0, 'points':[detections[0][0]]},\n",
        "    {'start_frame': 0, 'points':[detections[0][1]]},\n",
        "]\n",
        "rows, cols = scipy.optimize.linear_sum_assignment(cost)\n",
        "for r, c in zip(rows, cols):\n",
        "    tracks[r]['points'].append(detections[1][c])\n",
        "\n",
        "# Draw detections as dots\n",
        "N = len(detections)\n",
        "for i, coords in enumerate(detections):\n",
        "    x, y = xy_separate(coords)\n",
        "    g = i % 2\n",
        "    plt.scatter(x, y, color=(0, (1-g), g), s=300)\n",
        "\n",
        "# Draw line\n",
        "x, y = xy_separate(tracks[0]['points'])\n",
        "plt.plot(x, y)\n",
        "x, y = xy_separate(tracks[1]['points'])\n",
        "_ = plt.plot(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "642e0b4e-8067-4146-8406-ae95321f1817",
      "metadata": {
        "id": "642e0b4e-8067-4146-8406-ae95321f1817"
      },
      "source": [
        "## 4.3 Pulling association together\n",
        "\n",
        "We can measure the cost and use that to associate boxes using what we've learned.\n",
        "\n",
        "**Task**: Implement a function that associates a set of the last boxes of some tracks with a set of detections. You may choose to use Bounding box center distance or Bounding box overlap as your cost metric. *Hint*: A low cost implies a good match, so be careful with IoU, which is high for a good match."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1136a054-7666-4eea-a407-d014e293c09b",
      "metadata": {
        "id": "1136a054-7666-4eea-a407-d014e293c09b"
      },
      "outputs": [],
      "source": [
        "def associate(track_boxes, det_boxes):\n",
        "    '''\n",
        "    Find association indices between track_boxes and det_boxes.\n",
        "\n",
        "    Args:\n",
        "        track_boxes (np.array): last box of tracks shaped [N, 4] using tlx, tly, brx, bry\n",
        "        det_boxes (np.array): detections on this frame shaped [M, 4] using tlx, tly, brx, bry\n",
        "    Returns:\n",
        "        row_ind, col_ind: np.array output of scipy.optimize.linear_sum_assignment\n",
        "    '''\n",
        "    pass\n",
        "\n",
        "# These are from 4.1.3\n",
        "detections = [\n",
        "    [0.1, 0.1, 0.2, 0.2],\n",
        "    [0.3, 0.1, 0.5, 0.2],\n",
        "    [0.2, 0.5, 0.3, 0.7],\n",
        "    [0.6, 0.2, 0.9, 0.3],\n",
        "]\n",
        "tracks = [\n",
        "    {'start_frame': 0, 'boxes': [\n",
        "        [0.08, 0.08, 0.18, 0.18],\n",
        "        [0.09, 0.09, 0.19, 0.19],\n",
        "    ]},\n",
        "    {'start_frame': 0, 'boxes': [\n",
        "        [0.22, 0.52, 0.32, 0.72],\n",
        "        [0.21, 0.51, 0.31, 0.71],\n",
        "    ]},\n",
        "]\n",
        "detections_np = np.array(detections)\n",
        "previous_np = np.array([t['boxes'][-1] for t in tracks])\n",
        "rows, cols = associate(previous_np, detections_np)\n",
        "for r, c in zip(rows, cols):\n",
        "    tracks[r]['boxes'].append(detections[c])\n",
        "print(tracks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b7411bb-70ac-4b27-8914-84fe54ff821e",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "5b7411bb-70ac-4b27-8914-84fe54ff821e"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "def associate(track_boxes, det_boxes):\n",
        "    '''\n",
        "    Find association indices between track_boxes and det_boxes.\n",
        "\n",
        "    Args:\n",
        "        track_boxes (np.array): last box of tracks shaped [N, 4] using tlx, tly, brx, bry\n",
        "        det_boxes (np.array): detections on this frame shaped [M, 4] using tlx, tly, brx, bry\n",
        "    Returns:\n",
        "        row_ind, col_ind: np.array output of scipy.optimize.linear_sum_assignment()\n",
        "    '''\n",
        "    # Solution 1 - bounding box centers\n",
        "    track_points = np.array(tlbr_to_center(track_boxes))\n",
        "    det_points = np.array(tlbr_to_center(det_boxes))\n",
        "    cost = np.linalg.norm(track_points[:, None] - det_points[None], axis=-1)\n",
        "\n",
        "    # Solution 2 - bounding box IoU\n",
        "    cost = (1-bbox_iou_matrix(track_boxes, det_boxes))\n",
        "\n",
        "    return scipy.optimize.linear_sum_assignment(cost)\n",
        "\n",
        "# These are from 4.1.3\n",
        "detections = [\n",
        "    [0.1, 0.1, 0.2, 0.2],\n",
        "    [0.3, 0.1, 0.5, 0.2],\n",
        "    [0.2, 0.5, 0.3, 0.7],\n",
        "    [0.6, 0.2, 0.9, 0.3],\n",
        "]\n",
        "tracks = [\n",
        "    {'start_frame': 0, 'boxes': [\n",
        "        [0.08, 0.08, 0.18, 0.18],\n",
        "        [0.09, 0.09, 0.19, 0.19],\n",
        "    ]},\n",
        "    {'start_frame': 0, 'boxes': [\n",
        "        [0.22, 0.52, 0.32, 0.72],\n",
        "        [0.21, 0.51, 0.31, 0.71],\n",
        "    ]},\n",
        "]\n",
        "detections_np = np.array(detections)\n",
        "previous_np = np.array([t['boxes'][-1] for t in tracks])\n",
        "rows, cols = associate(previous_np, detections_np)\n",
        "for r, c in zip(rows, cols):\n",
        "    tracks[r]['boxes'].append(detections[c])\n",
        "print(tracks)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc15b27d-19e5-4d29-a918-9f10a5925263",
      "metadata": {
        "id": "dc15b27d-19e5-4d29-a918-9f10a5925263"
      },
      "source": [
        "# 5. Implementation\n",
        "\n",
        "Here we provide a simple tracking implementation that uses your `associate` function.\n",
        "\n",
        "Sometimes objects appear and disappear from view. When do we say a detection represents something new to have appeared? When do we say something has disappeared? These are difficult questions without a single, optimal answer. We'll take the simplest approach here. If we can't find a detection for a track on the next frame, it gets closed. If we can't find a track for a detection, we open a new track.\n",
        "\n",
        "**In your own time**: Look at another [simple implementation of tracking](https://github.com/jbencook/pytorch-object-tracking/blob/master/simple-track.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "634d8d5f-d8be-43f6-9e0c-91e44f560dca",
      "metadata": {
        "id": "634d8d5f-d8be-43f6-9e0c-91e44f560dca"
      },
      "outputs": [],
      "source": [
        "def do_tracking(detections):\n",
        "    ''' Track via detections '''\n",
        "    open_tracks = []\n",
        "    closed_tracks = []\n",
        "\n",
        "    for i, det_boxes in enumerate(detections):\n",
        "        track_indices = det_indices = []\n",
        "        if i > 0:\n",
        "            # Associate det_boxes with the open tracks\n",
        "            track_boxes = np.array([tracklet[\"boxes\"][-1] for tracklet in open_tracks])\n",
        "            track_indices, det_indices = associate(track_boxes, det_boxes)\n",
        "\n",
        "        # Add matches to open tracks\n",
        "        for track_idx, det_idx in zip(track_indices, det_indices):\n",
        "            open_tracks[track_idx][\"boxes\"].append(det_boxes[det_idx])\n",
        "\n",
        "        # Close lost tracks - tracks without a matching detection\n",
        "        lost_indices = set(range(len(open_tracks))) - set(track_indices)\n",
        "        for lost_idx in sorted(lost_indices, reverse=True):\n",
        "            closed_tracks.append(open_tracks.pop(lost_idx))\n",
        "\n",
        "        # Open new tracks - detections without a matching track\n",
        "        new_indices = set(range(len(det_boxes))) - set(det_indices)\n",
        "        for new_idx in new_indices:\n",
        "            open_tracks.append(\n",
        "                {\"start_frame\": i, \"boxes\": [det_boxes[new_idx]]}\n",
        "            )\n",
        "\n",
        "    return closed_tracks + open_tracks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "177da4ca-5739-491d-9e1d-157e4f483413",
      "metadata": {
        "id": "177da4ca-5739-491d-9e1d-157e4f483413"
      },
      "outputs": [],
      "source": [
        "all_tracks = do_tracking(all_detections)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1ad3152-24be-4161-b9b8-604fb04ff1f6",
      "metadata": {
        "id": "c1ad3152-24be-4161-b9b8-604fb04ff1f6"
      },
      "source": [
        "## 5.1 Visualise\n",
        "\n",
        "We will now draw the boxes and trajectories onto each frame of the video and visualise the tracking results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d090aab6-37ef-4ccb-92c5-5c32aeff6196",
      "metadata": {
        "id": "d090aab6-37ef-4ccb-92c5-5c32aeff6196"
      },
      "outputs": [],
      "source": [
        "vid = cv2.VideoCapture('./mot20-01.mp4')\n",
        "vid_length = int(vid.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
        "vid_out = cv2.VideoWriter('./mot20-01-det.mp4', fourcc, 15, (1920, 1080))\n",
        "\n",
        "for i in range(vid_length):\n",
        "    # Read a frame\n",
        "    _, img = vid.read()\n",
        "\n",
        "    # For all tracks, draw a box if it has a box on this frame\n",
        "    for t, track in enumerate(all_tracks):\n",
        "        # Find the idx within track for this frame\n",
        "        s_f = track['start_frame']\n",
        "        inner_idx = i-s_f\n",
        "        if 0 <= inner_idx < len(track['boxes']):\n",
        "            # Pull out box dimensions\n",
        "            tlx, tly, brx, bry = track['boxes'][inner_idx].astype(np.int32)\n",
        "            # Colour the box based on track index\n",
        "            colour = COLOURS[t%len(COLOURS)]\n",
        "            cv2.rectangle(img, (tlx, tly), (brx, bry), color=colour, thickness=2)\n",
        "\n",
        "    vid_out.write(img)\n",
        "\n",
        "vid_out.release()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download('./mot20-01-det.mp4')"
      ],
      "metadata": {
        "id": "IYm8MZyJoVRQ"
      },
      "id": "IYm8MZyJoVRQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7e6444bd-668f-4b54-9c69-5598ba8c2df1",
      "metadata": {
        "id": "7e6444bd-668f-4b54-9c69-5598ba8c2df1"
      },
      "source": [
        "This code creates a video out of the detections/tracks. Each track is assigned a colour. Try following individual people and see how often the boxes around them change colour. When it changes colour, that means it's lost them and created a new track. Each individual tends to have multiple tracks due to confounding extra detections and missed detections.\n",
        "\n",
        "**Question**: How would you rate the tracking?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Tutor's answer</u></summary>\n",
        "\n",
        "It is difficult to separate the detection quality from the tracking quality. The tracking isn't amazing. The people at the edges without much to obscure them tend to be tracked relatively well, but they still get lost a few times, with multiple tracks required to follow them. In the center, where there's lots of people. Each person's tracks only lasts a few seconds at best; there's too many confounding detections. It could be improved with a better tracking algorithm.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38c032b3-572c-45ba-ae83-3ccbdd56d59c",
      "metadata": {
        "id": "38c032b3-572c-45ba-ae83-3ccbdd56d59c"
      },
      "source": [
        "# 6 Velocity prediction\n",
        "\n",
        "Objects typically move from frame to frame. If we simply use euclidean distance, we run a serious risk of identity swapping. Consider the graph below. The changing colours represent the different frames that the detections are made."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d45f095-d966-46ee-a508-34baabb462b5",
      "metadata": {
        "tags": [],
        "id": "7d45f095-d966-46ee-a508-34baabb462b5"
      },
      "outputs": [],
      "source": [
        "detections = [\n",
        "    [[1, 1]],\n",
        "    [[2, 2]],\n",
        "    [[3, 3]],\n",
        "    [[4, 4], [3, 4]],\n",
        "    [[5, 5]],\n",
        "    [[6, 6]],\n",
        "    [[7, 7]],\n",
        "]\n",
        "\n",
        "# Draw dots\n",
        "N = len(detections)\n",
        "for i, coords in enumerate(detections):\n",
        "    x, y = xy_separate(coords)\n",
        "    g = i % 2\n",
        "    plt.scatter(x, y, color=(0, (1-g), g), s=300)\n",
        "\n",
        "# Draw line\n",
        "x = [p[0][0] for p in detections[:3]]\n",
        "y = [p[0][1] for p in detections[:3]]\n",
        "_ = plt.plot(x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c2340d3-82cd-4943-b9ed-b170cd432d79",
      "metadata": {
        "id": "1c2340d3-82cd-4943-b9ed-b170cd432d79"
      },
      "source": [
        "**Question**: The track represented in the above graph currently has `[(1,1), (2,2), (3,3)]`. It appears that the points are following linear motion. Assuming it is, out of `(3, 4)` and `(4, 4)`, which point is the most correct to add to this track?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "The point `(4, 4)` makes the most sense to add to this track, because it seems like the object is moving up and to the right at a steady rate. Notice, however that `(3, 4)` is closer to `(3, 3)` than `(4, 4)` is. So a method of association that does not account for motion will prefer to add `(3, 3)`.\n",
        "\n",
        "Predicting that the next point in the sequence is `(4, 4)` is called \"linear extrapolation\".\n",
        "</details>\n",
        "<br/>\n",
        "\n",
        "**Task**: Write a function `predict_next()` that makes a prediction for the next position for all tracks it is given using only the last two points of the track. If the track has only one point, then it should return that point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e85f700f-8bbd-4c32-97d8-828d13d6ef66",
      "metadata": {
        "id": "e85f700f-8bbd-4c32-97d8-828d13d6ef66"
      },
      "outputs": [],
      "source": [
        "# TODO write function\n",
        "# def predict_next(tracks):\n",
        "\n",
        "\n",
        "tracks = [\n",
        "    {'start_frame': 0, 'points': [\n",
        "        [1, 1],\n",
        "        [2, 2],\n",
        "        [3, 3],\n",
        "    ]},\n",
        "    {'start_frame': 0, 'points': [\n",
        "        [6, 1],\n",
        "        [5, 2],\n",
        "        [4, 3],\n",
        "    ]}\n",
        "]\n",
        "predicted_next = predict_next(tracks)\n",
        "assert predicted_next[0] == [4, 4]\n",
        "assert predicted_next[1] == [3, 4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15cf38fd-31bc-4f3a-8c95-b4e4ccda321b",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "cellView": "form",
        "id": "15cf38fd-31bc-4f3a-8c95-b4e4ccda321b"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "def predict_next(tracks):\n",
        "    predicted = []\n",
        "    for track in tracks:\n",
        "        if len(track['points']) == 1:\n",
        "            predicted.append(track['points'][0])\n",
        "        else:\n",
        "            # Convert to numpy array just to make the maths simpler\n",
        "            p_2 = np.array(track['points'][-2])\n",
        "            p_1 = np.array(track['points'][-1])\n",
        "            p_n = p_1 + (p_1-p_2)\n",
        "            predicted.append(p_n.tolist())\n",
        "    return predicted"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e269de3-d3bc-4b1b-b4cb-cf4da6818d18",
      "metadata": {
        "id": "4e269de3-d3bc-4b1b-b4cb-cf4da6818d18"
      },
      "source": [
        "## 6.1 Other methods to predict the next location\n",
        "\n",
        "Linear extrapolation using the last two points is the simplest method of estimating the next position for a track. Other methods for predicting the next point include:\n",
        " - Using momentum\n",
        " - Kalman filter\n",
        " - Neural network\n",
        "\n",
        "The most common method is the Kalman Filter. Exploring the Kalman Filter is outside the scope of this lab."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82c8e583-5714-4230-83d4-1b806f79cc8a",
      "metadata": {
        "id": "82c8e583-5714-4230-83d4-1b806f79cc8a"
      },
      "source": [
        "# 7. Aspects not covered\n",
        "\n",
        "Tracking suffers from being simple enough in a single case, but highly complex in the general case. There are a million different ways to describe the problem and there are a million different ways that a tracker can fail.\n",
        "\n",
        "Some issues with the above implementation in order of easy to solve to hard to solve:\n",
        "* It assumes all boxes are the same class.\n",
        "* Object velocity is ignored. Even using perfect detections, it will tend to swap identity when paths intersect.\n",
        "* It only uses points; this makes it faster and simpler, but theoretically a tracking algorithm that could look at the appearance of the detections should more accurate.\n",
        "* If an individual is completely missed for a single frame, it will either end the track, or jump to another individual.\n",
        "* The linear assignment is unconstrained. Imagine a single existing track on the left of the image, and a single new point detected on the right of the image. This point will be attached to that track, even though the distance is clearly too far to be the same object.\n",
        "* We have described a relatively greedy \"online\" tracking algorithm. That is, it runs iteratively and builds up tracks in a single pass of the data. This requires the least information. There are other tracking methods which take a more global approach to tracking, and use graphs to find a minimum cost association across the entire collection of detections which are theoretically more optimal.\n",
        "* Distances between objects in the foreground are treated the same as distances between objects in the background. Moving 10 pixels vertically in an image typically represents a larger physical distance than 10 pixels horizontally.\n",
        "\n",
        "There's a lot of variation between tracking algorithms; we have shown you just one way to do it.\n",
        "\n",
        "We also haven't touched evaluating tracking algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d72d6986-0936-470a-bb31-be772d423ca9",
      "metadata": {
        "id": "d72d6986-0936-470a-bb31-be772d423ca9"
      },
      "source": [
        "# Challenge Tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf2ad3c6-ecb0-4131-8c98-c05d6ab4c864",
      "metadata": {
        "id": "cf2ad3c6-ecb0-4131-8c98-c05d6ab4c864"
      },
      "source": [
        "## Challenge Task 1 - Solve aspects not covered\n",
        "\n",
        "Solve the first three issues in the above list.\n",
        "\n",
        "### Same class\n",
        "\n",
        "*Hint:* Store the detections per-class. Run tracking on each predicted class.\n",
        "\n",
        "### Velocity\n",
        "\n",
        "*Hint:* Instead of using the last position utilise linear extrapolation from the last two points of the track (if it exists) to predict the next location. The only change required is that you calculate the matrix created for the linear assignment problem differently.\n",
        "\n",
        "### Appearance features\n",
        "\n",
        "*Hint:* Use the technique shown in 4.1.4 to compare detections. Measure the \"distance\" between the track and the points as a balance between euclidean distance and appearance similarity. The only change required is that you calculate the matrix created for the linear assignment problem differently."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d246c91-f0b4-48c6-a2c4-7eb9be51701c",
      "metadata": {
        "id": "7d246c91-f0b4-48c6-a2c4-7eb9be51701c"
      },
      "source": [
        "## Challenge Task 2 - Use deep learning features\n",
        "\n",
        "We have previously noted that deep learning models have \"automatic feature extraction\". Then, if we want image features for something other than classification, one option is to leverage existing deep learning model's feature extraction. This is a common way to produce appearance features for matching in tracking.\n",
        "\n",
        "**Task**: Create an instance of the `mobilenet_v3` model from `torchvision`. Load the first frame of the video `mot20-01.mp4`. Get the maskrcnn detections for the first frame of the video. Use the first detection to crop the first frame of the video to only a single person. Call `preprocess_image()` from lab 3 on the crop, and give the result to the `mobilenet_v3.features()` function. Give the result of that to `mobilenet_v3.avgpool()`.\n",
        "\n",
        "**Question**: How many features does `mobilenet_v3` reduce the crop to after `avgpool()`? *Hint*: After `avgpool`, the shape will be in BCHW.\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "The output of `avgpool()` has 576 channels, and a height/width of 1. Thus there are 576 features.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e111cad-27cd-4537-95a0-00a9fbdf78f4",
      "metadata": {
        "id": "9e111cad-27cd-4537-95a0-00a9fbdf78f4"
      },
      "outputs": [],
      "source": [
        "# From Lab 3\n",
        "def preprocess_image(image):\n",
        "    image = tvtf.to_tensor(image)\n",
        "    image = tvtf.resize(image, 256)\n",
        "    image = tvtf.center_crop(image, 224)\n",
        "    image = tvtf.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    image = image.unsqueeze(dim=0)\n",
        "    return image\n",
        "\n",
        "# TODO: Get dl model\n",
        "\n",
        "\n",
        "# TODO: Get first frame of video\n",
        "\n",
        "\n",
        "# TODO: Get first maskrcnn detection of first frame\n",
        "\n",
        "\n",
        "# TODO: Crop first frame and prepare for model\n",
        "\n",
        "\n",
        "# TODO: Extract appearance features of crop using model\n",
        "\n",
        "print(dl_features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e33a2cd-956c-4291-b6f2-23772b2231de",
      "metadata": {
        "tags": [],
        "cellView": "form",
        "id": "7e33a2cd-956c-4291-b6f2-23772b2231de"
      },
      "outputs": [],
      "source": [
        "#@title Task solution\n",
        "\n",
        "# Get dl model\n",
        "mobilenet_v3 = torchvision.models.mobilenet_v3_small(pretrained=True)\n",
        "\n",
        "# Get first frame of video\n",
        "vid = cv2.VideoCapture('./mot20-01.mp4')\n",
        "_, first_frame = vid.read()\n",
        "\n",
        "# Get first maskrcnn detection of first frame\n",
        "first_det = all_detections[0][0].astype(np.int32)\n",
        "tlx, tly, brx, bry = first_det\n",
        "\n",
        "# Crop first frame  and prepare for model\n",
        "cropped = first_frame[tly:bry, tlx:brx]\n",
        "preprocessed = preprocess_image(cropped)\n",
        "\n",
        "# Extract features using model\n",
        "dl_features_0 = mobilenet_v3.features(preprocessed)\n",
        "dl_features = mobilenet_v3.avgpool(dl_features_0)\n",
        "print(dl_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0709843e-5abf-4147-b532-49a3755918af",
      "metadata": {
        "id": "0709843e-5abf-4147-b532-49a3755918af"
      },
      "source": [
        "# Summary\n",
        "\n",
        "In this lab we have described the core parts of tracking by detections, shown one simple implementation of tracking and visually evaluated using MaskRCNN with this tracking algorithm on a short video."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}