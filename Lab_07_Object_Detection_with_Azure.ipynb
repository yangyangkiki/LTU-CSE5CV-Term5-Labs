{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yangyangkiki/LTU-CSE5CV-Term5-Labs/blob/main/Lab_07_Object_Detection_with_Azure.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "495d6d98-a710-4e71-9dec-218fc1438e2b",
      "metadata": {
        "id": "495d6d98-a710-4e71-9dec-218fc1438e2b"
      },
      "source": [
        "# CSE5CV - Object Detection with Azure and the Microsoft Video Analyzer\n",
        "\n",
        "In this weeks lab we'll be training an object detection model with Microsoft Azure. In addition, we'll also learn how to use the Microsoft Video Analyzer."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Colab preparation\n",
        "\n",
        "Google Colab is a free online service for editing and running code in notebooks like this one. To get started, follow the steps below:\n",
        "\n",
        "1. Click the \"Copy to Drive\" button at the top of the page. This will open a new tab with the title \"Copy of...\". This is a copy of the lab notebook which is saved in your personal Google Drive. **Continue working in that copy, otherwise you will not be able to save your work**. You may close the original Colab page (the one which displays the \"Copy to Drive\" button).\n",
        "2. Run the code cell below to prepare the Colab coding environment by downloading sample files. Note that if you close this notebook and come back to work on it again later, you will need to run this cell again."
      ],
      "metadata": {
        "id": "qHrEAKS4h2Nm"
      },
      "id": "qHrEAKS4h2Nm"
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/ltu-cse5cv/cse5cv-labs.git\n",
        "%cd cse5cv-labs/Lab10"
      ],
      "metadata": {
        "id": "wM8UKEozh4MK"
      },
      "execution_count": null,
      "outputs": [],
      "id": "wM8UKEozh4MK"
    },
    {
      "cell_type": "markdown",
      "id": "286b5f42-47d3-412d-8b4d-6e8848fc847b",
      "metadata": {
        "id": "286b5f42-47d3-412d-8b4d-6e8848fc847b"
      },
      "source": [
        "## Packages\n",
        "\n",
        "We'll be using similar packages to those in the last lab."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install azure-cognitiveservices-vision-customvision~=3.1.0"
      ],
      "metadata": {
        "id": "g12W-5ePrFdn"
      },
      "execution_count": null,
      "outputs": [],
      "id": "g12W-5ePrFdn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9af4500b-1bcf-4b46-9434-aa9e22b92864",
      "metadata": {
        "id": "9af4500b-1bcf-4b46-9434-aa9e22b92864"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from azure.cognitiveservices.vision.customvision.prediction import CustomVisionPredictionClient\n",
        "from msrest.authentication import ApiKeyCredentials\n",
        "\n",
        "# Utility functions\n",
        "def load_image_rgb(filepath):\n",
        "    image = cv2.imread(filepath)\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    return image\n",
        "\n",
        "def display_image(image, title=None):\n",
        "    fig, axes = plt.subplots(figsize=(18, 12))\n",
        "\n",
        "    if image.ndim == 2:\n",
        "        axes.imshow(image, cmap='gray', vmin=0, vmax=255)\n",
        "    else:\n",
        "        axes.imshow(image)\n",
        "\n",
        "    axes.axis('off')\n",
        "\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Some useful functions to help overlay detections (Taken from Lab 9)\n",
        "COLOURS = [\n",
        "    tuple(int(colour_hex.strip('#')[i:i+2], 16) for i in (0, 2, 4))\n",
        "    for colour_hex in plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
        "]\n",
        "\n",
        "def draw_detections(img, dets, colours=COLOURS):\n",
        "    for i, (cl, tlx, tly, brx, bry) in enumerate(dets):\n",
        "        i %= len(colours)\n",
        "        cv2.rectangle(img, (tlx, tly), (brx, bry), color=colours[i], thickness=2)\n",
        "\n",
        "def annotate_class(img, dets, conf=None, colours=COLOURS):\n",
        "    for i, (cl, tlx, tly, brx, bry) in enumerate(dets):\n",
        "        txt = cl\n",
        "        if conf is not None:\n",
        "            txt += f' {conf[i]:1.3f}'\n",
        "        # A box with a border thickness draws half of that thickness to the left of the\n",
        "        # boundaries, while filling fills only within the boundaries, so we expand the filled\n",
        "        # region to match the border\n",
        "        offset = 1\n",
        "\n",
        "        cv2.rectangle(img,\n",
        "                      (tlx-offset, tly-offset-12),\n",
        "                      (tlx-offset+len(txt)*12, tly),\n",
        "                      color=colours[i],\n",
        "                      thickness=cv2.FILLED)\n",
        "\n",
        "        ff = cv2.FONT_HERSHEY_PLAIN\n",
        "        cv2.putText(img, txt, (tlx, tly-1), fontFace=ff, fontScale=1.0, color=(255,)*3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ef3552a-48eb-4a15-b2ed-7745d0002051",
      "metadata": {
        "id": "9ef3552a-48eb-4a15-b2ed-7745d0002051"
      },
      "source": [
        "# 1. Creating Resources\n",
        "\n",
        "Just like in the previous lab, before we can work with Azure we need to setup a \"resource\".\n",
        "\n",
        "We will be using the Custom Vision cognitive service to perform object detection in Azure. Because we will both train a model and use it for inference on new data, we'll need to setup resources to do this (specifically a \"Custom Vision\" resource).\n",
        "\n",
        "**Remember**: Before finishing this lab, make sure you remove the Custom Vision resource you create (see Section 1.2)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ea673d6-cac5-4212-b2a2-86e2eea43748",
      "metadata": {
        "id": "0ea673d6-cac5-4212-b2a2-86e2eea43748"
      },
      "source": [
        "## 1.1 Create Custom Vision Resource\n",
        "\n",
        "**Task**: Create a \"Custom Vision\" resource by following the instructions below.\n",
        "\n",
        "Starting from: https://portal.azure.com:\n",
        "\n",
        "1. Click \"Create a Resource\"\n",
        "2. Search for \"Custom Vision\"\n",
        "3. Select \"Custom Vision\" and then click \"Create\"\n",
        "4. Create a new resource group by clicking \"Create new\" under \"Resource Group\". Name it \"CSE5CV-Azure\".\n",
        "5. Enter a unique instance name including your student ID and \"customvision\". For example, `22222222customvision`.\n",
        "6. Select the free tier for both training and prediction.\n",
        "7. Click \"Review + Create\". Then click \"Create\"."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2f3e674-331e-4b4b-8655-a11fde13979c",
      "metadata": {
        "id": "f2f3e674-331e-4b4b-8655-a11fde13979c"
      },
      "source": [
        "## 1.2 Deleting Resource Groups\n",
        "\n",
        "**Important**: **To do only when you are done with this lab**\n",
        "\n",
        "*When you are done with this lab* you can follow these instructions to delete your resource group. This will remove all resources you created in one go.\n",
        "\n",
        "1. Visit https://portal.azure.com/#blade/HubsExtension/BrowseResourceGroups.\n",
        "2. Select the resource group (click the name)\n",
        "3. Click \"Delete resource group\"\n",
        "4. Enter the name \"CSE5CV-Azure\" (this is a clever UI to ensure that you are deleting the resource group you intend to).\n",
        "5. Click \"Delete\"."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "672fc2ff-aa0b-43d5-b91c-7f86202108ae",
      "metadata": {
        "id": "672fc2ff-aa0b-43d5-b91c-7f86202108ae"
      },
      "source": [
        "## 1.3 Collecting Communication Credentials\n",
        "\n",
        "These resources come pre-configured to receive REST requests. However, they are not open to the world; you need to use specific credentials to access them. Here we describe how to get the credentials for your Custom Vision resource."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7263170f-43fe-4361-ba92-995abf9e611a",
      "metadata": {
        "id": "7263170f-43fe-4361-ba92-995abf9e611a"
      },
      "source": [
        "We will models for the Custom Vision resource to use for prediction. We will choose which model to access using the project id and model's name. However, all of the REST requests will go to the same resource using the same credentials.\n",
        "\n",
        "**Task**: Collect your Custom Vision resource's REST credentials from Azure.\n",
        "\n",
        "1. Visit: https://www.customvision.ai\n",
        "2. Log in with your La Trobe Student account.\n",
        "3. Click the cog in the top right to view your resources.\n",
        "4. Open your \"Prediction\" resource to see the *Prediction endpoint* and the *Prediction key*.\n",
        "5. Record the endpoint and the key in the following code cell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb9d9f55-0861-4bd8-b96f-f57dfbf5aadc",
      "metadata": {
        "id": "eb9d9f55-0861-4bd8-b96f-f57dfbf5aadc"
      },
      "outputs": [],
      "source": [
        "# TODO: fill with your data\n",
        "custom_vision_endpoint = ''\n",
        "custom_vision_key = ''\n",
        "\n",
        "custom_vision_credentials = ApiKeyCredentials(\n",
        "    in_headers={\"Prediction-key\": custom_vision_key}\n",
        ")\n",
        "custom_vision_client = CustomVisionPredictionClient(\n",
        "    endpoint=custom_vision_endpoint,\n",
        "    credentials=custom_vision_credentials\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6815fbb5-e716-4bd6-8ec1-29abf56eaa8d",
      "metadata": {
        "id": "6815fbb5-e716-4bd6-8ec1-29abf56eaa8d"
      },
      "source": [
        "# 2. Fruit Object Detection\n",
        "\n",
        "Using the Custom Vision resource we created, we will train a model that can detect fruit within an image. To do this, we will annotate images of fruit using the Azure interface. Once we train the model, we will deploy it onto our Custom Vision resource, and make predictions on images from within this notebook using a REST API."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b37eb30-bc9e-411d-90d1-87ddd6feafde",
      "metadata": {
        "id": "9b37eb30-bc9e-411d-90d1-87ddd6feafde"
      },
      "source": [
        "## 2.1 Create a New Project\n",
        "\n",
        "To get started, we first need to create a new project.\n",
        "\n",
        "Visit: https://customvision.ai and sign in using the Microsoft account associated with your Azure subscription.\n",
        "\n",
        "**Task**: Create a new project with the following settings:\n",
        "\n",
        "* **Name**: Grocery Detection\n",
        "* **Description**: Object detection for groceries\n",
        "* **Resource**: (Select the name of the Custom Vision resource you created earlier)\n",
        "* **Project Types**: Object Detection\n",
        "* **Domains**: General\n",
        "\n",
        "After clicking \"Create Project\" the project will automatically open in your browser."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6294463a-d52d-475b-9da4-b721415f560f",
      "metadata": {
        "id": "6294463a-d52d-475b-9da4-b721415f560f"
      },
      "source": [
        "## 2.2 Upload Training Images\n",
        "\n",
        "To train our object detection model, we need to upload images that contain classes we want to detect and annotate them.\n",
        "\n",
        "**Task**: Upload all of your training images.\n",
        "1. Download and extract the following .zip archive containing fruit images: https://github.com/ltu-cse5cv/cse5cv-labs/releases/download/v0.0.0/fruitdet.zip\n",
        "2. Back on the Custom Vision web page, click \"Add images\".\n",
        "3. Navigate to `fruitdet/train`. Use shift to select all the images in this folder and upload them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5701f5d1-bda9-416d-b365-8a68005837e8",
      "metadata": {
        "id": "5701f5d1-bda9-416d-b365-8a68005837e8"
      },
      "source": [
        "**Question**: What things do you think we should annotate for this task?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "For every piece of fruit in the image, we should annotate:\n",
        "* The class of the fruit (e.g. apple, banana, orange)\n",
        "* The bounding box coordinates of the fruit (e.g. tlx, tly, brx, bry coordinates)\n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0727fa25-588b-40ce-8cda-31e56b503103",
      "metadata": {
        "id": "0727fa25-588b-40ce-8cda-31e56b503103"
      },
      "source": [
        "## 2.3 Annotating the Images\n",
        "\n",
        "After all images have been uploaded, the next step is to annotate them.\n",
        "\n",
        "**Task**: Annotate all training images that you have uploaded.\n",
        "\n",
        "1. Click on the first image that has been uploaded. Once you do this, you should see an \"Image Detail\" screen.\n",
        "2. There are two ways you can draw boxes around objects in the image:\n",
        "    * Click and drag your mouse to draw a box around one of the objects\n",
        "    * Hover your mouse over any of the objects in the image until an automatically detected region is displayed like below. Click the region once it is displayed.\n",
        "3. Once the region is selected, resize the region as necessary to capture the whole object, then assign a tag (class) to the object. This should be: *apple*, *banana*, or *orange*.\n",
        "4. Repeat this process to annotate all objects within the image.\n",
        "5. Use the **>** button on the right to go to the next image, and tag its objects. Keep working through the entire set of images, tagging each *apple*, *banana*, and *orange*.\n",
        "6. Once you've finished tagging the last image, close the **Image Detail** editor and in the left hand panel, under **Tags**, select **Tagged** to see all of your tagged images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "271854fa-2b61-415b-bcc1-6872dd79fe50",
      "metadata": {
        "id": "271854fa-2b61-415b-bcc1-6872dd79fe50"
      },
      "source": [
        "**Question**: How many examples of each class do we have?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "Across the 33 images, you should have:\n",
        "* 18 apples\n",
        "* 20 bananas\n",
        "* 20 oranges\n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "257bfa15-7dd8-457d-a8d6-47445de218bb",
      "metadata": {
        "id": "257bfa15-7dd8-457d-a8d6-47445de218bb"
      },
      "source": [
        "## 2.4 Train Model\n",
        "\n",
        "Now that the dataset is fully annotated, you're ready to train a model.\n",
        "\n",
        "**Task**: Click the green \"Train\" button in the top-right. Choose \"Quick Training\". Click \"Train\".\n",
        "\n",
        "While it is training, we will collect the project id.\n",
        "\n",
        "**Task**: Click the cog in the top-right to get to the project settings. Copy the \"Project ID\" into the `project_id` variable in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06d5fedc-28ce-442c-8cff-965d482f3fbd",
      "metadata": {
        "id": "06d5fedc-28ce-442c-8cff-965d482f3fbd"
      },
      "outputs": [],
      "source": [
        "# TODO: Fill in project id\n",
        "project_id = ''"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc06aa3b-68b1-44fa-ac67-a2530d5f2317",
      "metadata": {
        "id": "dc06aa3b-68b1-44fa-ac67-a2530d5f2317"
      },
      "source": [
        "**Task**: Click \"Performance\" in the top-right. This will take about 10 minutes to complete training. While you're waiting for the model to train, do the task at the start of [Section 3](#3.-Microsoft-Video-Analyzer), them come back here."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d75ee56b-7dd9-4cf0-b6b2-4ca3ad7660d0",
      "metadata": {
        "id": "d75ee56b-7dd9-4cf0-b6b2-4ca3ad7660d0"
      },
      "source": [
        "## 2.5 Evaluate Model\n",
        "\n",
        "When your model has finished training you will be presented with a page showing precision, recall, and AP performance metrics."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3919202a-dae8-4516-a9cc-92a6ed4b4f93",
      "metadata": {
        "id": "3919202a-dae8-4516-a9cc-92a6ed4b4f93"
      },
      "source": [
        "**Question**: In the context of object detection, how do we know if a detection is a true positive?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "A detection is considered as a true positive if the Intersection over Union (IoU) between the detection and ground truth is greater than some threshold. By default, Azure has set this threshold to 30% (See the left hand panel).\n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce479af3-0178-4122-a4e3-995a88a18167",
      "metadata": {
        "id": "ce479af3-0178-4122-a4e3-995a88a18167"
      },
      "source": [
        "Before deploying our model, let's see how we can use the Azure interface to test our newly trained model.\n",
        "\n",
        "\n",
        "**Task**: Click the \"Quick Test\" button in the top-right and upload the image found at `fruitdet/test/apple_orange.png` (Click \"Browse local files\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f807469c-eb6c-4d8e-9ea4-5f376888b217",
      "metadata": {
        "id": "f807469c-eb6c-4d8e-9ea4-5f376888b217"
      },
      "source": [
        "**Question**: How were the predictions? Were the class predictions correct? Did the boxes bound the objects well?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "Tutors answer: The trained model detected both the apple and orange well, both with high confidence (~97% and ~95% respectively). The detected locations also covered the objects well.\n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75985c90-43f2-44a6-9db1-4657e916ae1c",
      "metadata": {
        "id": "75985c90-43f2-44a6-9db1-4657e916ae1c"
      },
      "source": [
        "Whilst still in Azure, let's also test on an image we can find on the web.\n",
        "\n",
        "Take a look at the image here: https://unsplash.com/photos/DapP9j2DJMQ. How do you think your detection model will perform?\n",
        "\n",
        "**Task**: In the \"Quick Test\" window, paste the following URL into the \"Image URL\" section (This is a direct link to the image shown just before): https://images.unsplash.com/photo-1507260385058-676ee3f043e3?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=1400&q=80"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eac2cb6c-a812-4249-8306-0bad7ccb43f1",
      "metadata": {
        "id": "eac2cb6c-a812-4249-8306-0bad7ccb43f1"
      },
      "source": [
        "**Question**: How were the predictions?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "Tutors answer: My model was able to detect all 3 apples with reasonably high confidence (lowest was ~75%), but your results may vary.\n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ffa44c3-8365-4073-915c-0440c47b12fc",
      "metadata": {
        "id": "1ffa44c3-8365-4073-915c-0440c47b12fc"
      },
      "source": [
        "## 2.5 Deploy model\n",
        "\n",
        "**Task**: Navigate back to the \"Performance\" page and click \"Publish\" (in the top-left corner). Use the following settings:\n",
        "* **Model name**: detect-produce\n",
        "* **Prediction resource**: (the name of your resource ending in \"-Prediction\")\n",
        "\n",
        "Then click \"Publish\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32e457fd-523d-424a-b614-b1026c3d2788",
      "metadata": {
        "id": "32e457fd-523d-424a-b614-b1026c3d2788"
      },
      "source": [
        "In the next cell we interact with the model that we just trained and published/deployed on our resource. Publishing is instant; you can run the next cell right away."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d1bd37e-5213-4409-bbfb-8ba1fb482833",
      "metadata": {
        "id": "6d1bd37e-5213-4409-bbfb-8ba1fb482833"
      },
      "outputs": [],
      "source": [
        "model_name = 'detect-produce'\n",
        "\n",
        "with open(Path('fruitdet', 'test', 'produce.jpg'), 'rb') as f:\n",
        "    response = custom_vision_client.detect_image(project_id, model_name, f.read())\n",
        "\n",
        "# The detections are returned in descending order of probability\n",
        "# Here, we inspect the most confident detection\n",
        "print('The whole response:    -------')\n",
        "print(response)\n",
        "print('------------------------------')\n",
        "print()\n",
        "print('Most confident detection: ', response.predictions[0])\n",
        "print('Associated bounding box: ', response.predictions[0].bounding_box)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "011c8cca-e63c-4926-b91c-2c6480c15fc9",
      "metadata": {
        "id": "011c8cca-e63c-4926-b91c-2c6480c15fc9"
      },
      "source": [
        "**Question**: Looking at the bounding box information returned for the most confident detection, what form are the bounding box coordinates in?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "The keys of the bounding box data consist of: *left*, *top*, *width*, and *height*, so the form of the bounding box is in `tlx`, `tly`, `bw`, `bh` form.\n",
        "    \n",
        "Additionally, the coordinates are normalized to the image dimensions (You can tell this because the coordinates are in the range [0, 1])\n",
        "    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88986012-2f06-4141-af4b-d666ab06767f",
      "metadata": {
        "id": "88986012-2f06-4141-af4b-d666ab06767f"
      },
      "source": [
        "## 2.6 Test Model\n",
        "\n",
        "Now we will visually evaluate another test image, this time with Python!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c94eda7d-414d-4d8f-93fa-018dbbbe3bf2",
      "metadata": {
        "id": "c94eda7d-414d-4d8f-93fa-018dbbbe3bf2"
      },
      "outputs": [],
      "source": [
        "test_image_path = Path('fruitdet', 'test', 'produce.jpg')\n",
        "conf_thresh = 0.5          # Only overlay detections with a confidence score > this\n",
        "\n",
        "# Detect objects in image\n",
        "with open(test_image_path, 'rb') as f:\n",
        "    results = custom_vision_client.detect_image(project_id, model_name, f.read())\n",
        "\n",
        "# Load the image data (and determine dimensions)\n",
        "image = load_image_rgb(str(test_image_path))\n",
        "im_h, im_w = image.shape[:2]\n",
        "\n",
        "class_colours = {\n",
        "    'apple': (255, 0, 0),\n",
        "    'banana': (187, 88, 149),\n",
        "    'orange': (0, 0, 255),\n",
        "}\n",
        "\n",
        "# Overlay all detected boxes\n",
        "if results.predictions:\n",
        "    objs = []\n",
        "    confs = []\n",
        "    colours = []\n",
        "    for detection in results.predictions:\n",
        "        if detection.probability > conf_thresh:\n",
        "            box = detection.bounding_box\n",
        "            tlx, tly, brx, bry = box.left, box.top, box.left + box.width, box.top + box.height\n",
        "            tlx, tly, brx, bry = int(tlx * im_w), int(tly * im_h), int(brx * im_w), int(bry * im_h)\n",
        "            objs.append((detection.tag_name, tlx, tly, brx, bry))\n",
        "            confs.append(detection.probability)\n",
        "            colours.append(class_colours[detection.tag_name])\n",
        "    draw_detections(image, objs, colours=colours)\n",
        "    annotate_class(image, objs, conf=confs, colours=colours)\n",
        "\n",
        "\n",
        "# Display the image\n",
        "display_image(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37222732-8cd7-48ea-bce7-d39785b500fd",
      "metadata": {
        "id": "37222732-8cd7-48ea-bce7-d39785b500fd"
      },
      "source": [
        "**Question**: Looking at the response in the deploy model section, it appeared that the list of predictions was very large (quite a lot larger than 3). Why do we only see 3 detections overlaid onto the image?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "Because our confidence threshold is set to 0.5, all other predictions have been filtered out. Set `conf_thresh` to 0 and look at all detections that have been overlaid.\n",
        "    \n",
        "This is a good example showing the importance of setting a confidence threshold on the predictions of your model. Choosing a good threshold can be a tricky process, and you will find that when choosing this threshold, there is a tradeoff between precision and recall.    \n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecacfc0a-d68c-4dcf-bb39-25ca4f9f9462",
      "metadata": {
        "id": "ecacfc0a-d68c-4dcf-bb39-25ca4f9f9462"
      },
      "source": [
        "## 2.7 Summary\n",
        "\n",
        "You used www.customvision.ai to upload and annotate a training set for object detection, which you used to train a model and deploy it on Azure infrastructure. This model was accessible through a REST client. Azure provides a simple wrapper for us to use to communicate: `custom_vision_client`. In each request, we provided: `project_id`, `model_name` and some image data, and got back a prediction. Finally, we checked that it was working by sending unseen images."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42def973-f283-4c98-bf62-02d77abe079a",
      "metadata": {
        "id": "42def973-f283-4c98-bf62-02d77abe079a"
      },
      "source": [
        "## 2.8 Remove Project\n",
        "\n",
        "The free tier only allows up to 2 projects active at once. Here are the instructions for removing this project.\n",
        "\n",
        "1. Unpublish all of your models\n",
        "2. Click the eye in the top left\n",
        "3. Hover your project and click the trash can"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63524131-3656-4bc1-8094-03e955847bdd",
      "metadata": {
        "id": "63524131-3656-4bc1-8094-03e955847bdd"
      },
      "source": [
        "# 3. Microsoft Video Analyzer\n",
        "\n",
        "In Lab 9, we saw how we could provide an image to a Microsoft Azure Cognitive Services to perform a suite of analysis options. Now, we will do something similar using Video, but through an entirely new interface. This new interface is not tied to Azure directly, and you cannot use a La Trobe student account to do this. If you do not have a personal Microsoft account, you should make one. You do not need any credits or a special subscription for this section.\n",
        "\n",
        "There is no need to clean up anything after this task.\n",
        "\n",
        "**Task**: Analyze a video with Microsoft Video Analyzer.\n",
        " 1. Visit www.videoindexer.ai\n",
        " 2. Sign in with \"Personal Microsoft account\"\n",
        " 3. Click \"Upload\"\n",
        " 4. Click \"Enter URL\"\n",
        " 5. Enter the URL: https://aka.ms/responsible-ai-video and click \"Add\".\n",
        " 6. Click \"Upload + index\".\n",
        " 7. It will take about 5-10 minutes to analyze. Go stand outside and look at a plant in your garden. I mean really look at it. See if there's any insects crawling on it. How many branches does it have? Or just wait at your computer if you prefer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "946806f0-f0a8-490e-a0e6-b05980dc67b2",
      "metadata": {
        "id": "946806f0-f0a8-490e-a0e6-b05980dc67b2"
      },
      "source": [
        "## 3.1 Observe Insights\n",
        "\n",
        "The analysis has detected many \"insights\".\n",
        "\n",
        "The insights listed are:\n",
        "\n",
        " 1. People: A list of the people shown in the video using facial features. Given the number of quick shots of groups of people, it is unlikely to have found every person. Some of the people have been identified. These are \"famous\" people that can be reliably searched for on the internet.\n",
        " 2. Topics: A list of topics of the video. Microsoft has determined their own categorisation of topics, and fit the video into them based on the transcription.\n",
        " 3. Keywords: A list of keywords, distinct from the topics, more free-form.\n",
        " 4. Labels: A list of objects that were detected in the video\n",
        " 5. Named entities: People and brands mentioned/shown in the video.\n",
        " 6. Scenes: Segmenting the video into distinct scenes/shots.\n",
        "\n",
        "Spend some time looking through the insights. Each insight shows where in the video timeline it was detected, and you can click on the timelines in the \"Insights\" pane to skip to that location. Look at the things it got correct and incorrect. The next section is a guided tour of some errors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9560eefd-9308-419d-b492-e3e6bc1f540f",
      "metadata": {
        "id": "9560eefd-9308-419d-b492-e3e6bc1f540f"
      },
      "source": [
        "## 3.2 Errors\n",
        "\n",
        "While it gets a lot right, it also makes some errors. It is useful to know in what ways this system will fail.\n",
        "\n",
        "**Error 1**: It missed many people. In particular, in the classroom shots, only the front-most students are detected. Perhaps this isn't an \"error\", since it finds the most prominent people well enough.\n",
        "\n",
        "**Error 2**: The scenes group shots haphazardly. Although the shots appear to be well detected, the scenes are apparently random collections of adjacent shots.\n",
        "\n",
        "What errors did you find?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d7089e8-8b0a-48f5-b8ac-e769a74c0d8c",
      "metadata": {
        "id": "7d7089e8-8b0a-48f5-b8ac-e769a74c0d8c"
      },
      "source": [
        "## 3.3 OCR\n",
        "\n",
        "During the video, there were a number of text elements that appeared to complement the narrators spoken words. Part of the indexing/analysis used OCR on the video. Let's see how it did.\n",
        "\n",
        "**Task**: List all the text detected with OCR.\n",
        "  1. In the \"Insights\" pane, click \"Timeline\".\n",
        "  2. Click the \"View\" dropdown.\n",
        "  3. Untick \"Transcript\".\n",
        "  4. Tick \"OCR\".\n",
        "  5. Click \"View\" again to hide the dropdown.\n",
        "\n",
        "**Question**: How well did it do?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "It appears to have done quite well overall! It finds the name plates for the people, and some of the code shown for a few seconds. It also recognises the 6 principles of AI ethics at 0:38.\n",
        "</details>\n",
        "<br />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd7d8493-19c1-427f-8423-a31d6996a922",
      "metadata": {
        "id": "dd7d8493-19c1-427f-8423-a31d6996a922"
      },
      "source": [
        "##  3.4 Content Searching\n",
        "\n",
        "Another useful feature of the Microsoft Video Analyzer is the ability to search your video for content by keyword. For example. Do you remember where the 2 second shot of a bee appears in the video? Let's use the Insights to find it.\n",
        "\n",
        "**Task**: Find where the bee appears in the video.\n",
        "  1. Ensure \"Insights\" is selected on the right-hand pane.\n",
        "  2. Search \"bee\".\n",
        "  3. Scroll to \"Labels\" in the insights.\n",
        "\n",
        "**Question**: What timestamp did the bee appear?\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "The bee appears at 0:06 for about 2s.\n",
        "</details>\n",
        "<br />\n",
        "\n",
        "**Question**: Name 3 other \"labels\" that exclusively point to this shot of a bee. (You will have to clear \"bee\" from the search to see the full list of labels again)\n",
        "\n",
        "<details>\n",
        "<summary style='cursor:pointer;'><u>Answer</u></summary>\n",
        "\n",
        "I could find: \"invertibrate\", \"insect\", \"fly\", \"arthropod\", \"pest\", \"animal\", \"membrane-winged insect\", \"macro photography\" and \"net-winged insects\". There are perhaps more.\n",
        "\n",
        "This is good. You could use any of those words in your search to find the bee.\n",
        "</details>\n",
        "<br />"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f3893d5-4ea9-46bb-bce4-4574ee7692a7",
      "metadata": {
        "id": "3f3893d5-4ea9-46bb-bce4-4574ee7692a7"
      },
      "source": [
        "## 3.5 Microsoft Video Analyzer Summary\n",
        "\n",
        "We uploaded a video to Microsoft Video Analyzer, observed it's insights and qualitatively evaluated it's results."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bf111d4d-e2a8-4d17-9428-d604a35c04ae",
      "metadata": {
        "id": "bf111d4d-e2a8-4d17-9428-d604a35c04ae"
      },
      "source": [
        "# Reminder: Shut down resource group\n",
        "\n",
        "Make sure to close down the resource group for section 2 when you are done. See section 1.2 for instructions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c989814-ddc9-4a85-8d23-fbaf2abc9a9d",
      "metadata": {
        "id": "1c989814-ddc9-4a85-8d23-fbaf2abc9a9d"
      },
      "source": [
        "# Summary\n",
        "\n",
        "In this lab, you saw how to train a custom object detection model using Microsoft Azure with Custom Vision resources. In addition, you also learned how to use the Microsoft Video Analyzer."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}